/**
 * @description OpenAI API ì‚¬ìš©ì„ ìœ„í•œ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  ê´€ë¦¬í•˜ëŠ” ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
 * ë‹¤ì–‘í•œ ë¬¸ì ìœ í˜•(URL, ì´ëª¨ì§€, í•œê¸€, ì˜ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì ë“±)ì— ëŒ€í•œ í† í° ìˆ˜ë¥¼ ì¶”ì •í•˜ê³ ,
 * í…ìŠ¤íŠ¸ ë¶„í•  ë° ìµœì í™” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
 * 
 * @class
 * 
 * @property {Object} urlPatterns - URL íŒ¨í„´ë³„ í† í° ê°€ì¤‘ì¹˜ ì •ë³´
 *   - key: URL íŒ¨í„´ ë¬¸ìì—´ (ì˜ˆ: 'http://', '.com')
 *   - value: í•´ë‹¹ íŒ¨í„´ì˜ í† í° ê°€ì¤‘ì¹˜
 * 
 * @throws {TypeError} ì˜ëª»ëœ íƒ€ì…ì˜ ì…ë ¥ê°’ ì œê³µ ì‹œ
 * @throws {Error} í† í° ì œí•œê°’ì´ 1 ë¯¸ë§Œì¸ ê²½ìš°
 * 
 * @see https://platform.openai.com/tokenizer - OpenAI í† í° ê³„ì‚°ê¸°
 * 
 * @example ê¸°ë³¸ í† í° ê³„ì‚°
 * const text = "ì•ˆë…•í•˜ì„¸ìš”! Hello World ğŸ‘‹";
 * const tokenCount = TokenCounter.getEstimatedTokenCount(text);
 * console.log(tokenCount); // ì˜ˆìƒ í† í° ìˆ˜ ì¶œë ¥
 * 
 * @example í† í° ì œí•œì— ë”°ë¥¸ í…ìŠ¤íŠ¸ ë¶„í• 
 * const longText = "ì´ê²ƒì€ ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.";
 * const chunks = TokenCounter.splitByTokenLimit(longText, 10);
 * for (const chunk of chunks) {
 *   console.log(chunk); // ë¶„í• ëœ í…ìŠ¤íŠ¸ ì²­í¬ ì¶œë ¥
 * }
 * 
 * @example í† í° ìµœì í™”
 * const text = "ìì„¸í•œ ë‚´ìš©ì€ https://example.com/very/long/path ì°¸ê³ í•˜ì„¸ìš”.";
 * const optimized = TokenCounter.optimizeToTokenLimit(text, 8);
 * console.log(optimized); // ìµœì í™”ëœ í…ìŠ¤íŠ¸ ì¶œë ¥
 * 
 * @note
 * - í† í° ìˆ˜ ê³„ì‚°ì€ ì¶”ì •ì¹˜ì´ë©°, ì‹¤ì œ OpenAI API ì‚¬ìš©ì‹œì™€ ì•½ 3% ë‚´ì™¸ì˜ ì˜¤ì°¨ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
 * - URLì€ ë„ë©”ì¸ê³¼ ê²½ë¡œë¥¼ ë¶„ë¦¬í•˜ì—¬ ê³„ì‚°ë˜ë©°, ê¸´ URLì€ ìë™ìœ¼ë¡œ ì¶•ì•½ë©ë‹ˆë‹¤
 * - í•œê¸€ì€ ìŒì ˆ ë‹¨ìœ„ë¡œ ê³„ì‚°ë˜ë©°, ê¸¸ì´ì— ë”°ë¼ ê°€ì¤‘ì¹˜ê°€ ì ìš©ë©ë‹ˆë‹¤
 * - ì—°ì†ëœ íŠ¹ìˆ˜ë¬¸ìëŠ” ì¶•ì†Œëœ í† í° ìˆ˜ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤
 * - í…ìŠ¤íŠ¸ ë¶„í•  ì‹œ ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ ìµœëŒ€í•œ ë³´ì¡´í•˜ë ¤ ì‹œë„í•©ë‹ˆë‹¤
 */
class TokenCounter {
    /**
     * @description OpenAI API ì‚¬ìš©ì„ ìœ„í•œ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ì¶”ì •í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.
     * URL, ì´ëª¨ì§€, í•œê¸€, ì˜ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì ë“± ë‹¤ì–‘í•œ ë¬¸ì ìœ í˜•ì„ ê³ ë ¤í•˜ì—¬ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
     * ì‹¤ì œ OpenAIì˜ í† í° ê³„ì‚°ê³¼ ë¹„êµí•˜ì—¬ ì•½ 3% ë‚´ì™¸ì˜ ì˜¤ì°¨ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
     * 
     * @param {string} text - í† í° ìˆ˜ë¥¼ ê³„ì‚°í•  í…ìŠ¤íŠ¸
     *   - URL, ì´ëª¨ì§€, í•œê¸€, ì˜ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì ë“± ëª¨ë“  ìœ í˜•ì˜ í…ìŠ¤íŠ¸ ì…ë ¥ ê°€ëŠ¥
     *   - ë¹ˆ ë¬¸ìì—´ ë˜ëŠ” ê³µë°±ë§Œ ìˆëŠ” ê²½ìš° 0ì„ ë°˜í™˜
     * 
     * @returns {number} ì¶”ì •ëœ í† í° ìˆ˜ (1 ì´ìƒì˜ ì •ìˆ˜)
     * 
     * @see _processUrl - URL í† í° ê³„ì‚°
     * @see _processEmojis - ì´ëª¨ì§€ í† í° ê³„ì‚°
     * @see _processKorean - í•œê¸€ í† í° ê³„ì‚°
     * @see _processAlphanumeric - ì˜ìˆ«ì í† í° ê³„ì‚°
     * @see _processSpecialChars - íŠ¹ìˆ˜ë¬¸ì í† í° ê³„ì‚°
     * 
     * @example ê¸°ë³¸ í…ìŠ¤íŠ¸ í† í° ê³„ì‚°
     * TokenCounter.getEstimatedTokenCount("ì•ˆë…•í•˜ì„¸ìš”!");
     * // ê²°ê³¼: 3 (í•œê¸€ ë¬¸ì¥)
     * 
     * @example ë³µí•© í…ìŠ¤íŠ¸ í† í° ê³„ì‚°
     * TokenCounter.getEstimatedTokenCount("Hello World! ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ https://example.com");
     * // ê²°ê³¼: 11 (ì˜ë¬¸ + í•œê¸€ + ì´ëª¨ì§€ + URL)
     * 
     * @note
     * - í† í° ìˆ˜ ê³„ì‚°ì€ ì¶”ì •ì¹˜ì´ë©°, ì‹¤ì œ OpenAI API ì‚¬ìš©ì‹œì˜ í† í° ìˆ˜ì™€ ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
     * - ê¸´ ë¬¸ì¥ì˜ ê²½ìš° ë¬¸ì¥ ë¶€í˜¸(., !, ?)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤
     * - URLì€ ë„ë©”ì¸ê³¼ ê²½ë¡œë¥¼ ë¶„ë¦¬í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤
     * - ì—°ì†ëœ íŠ¹ìˆ˜ë¬¸ìëŠ” ì¶•ì†Œëœ í† í° ìˆ˜ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤
     */
    static getEstimatedTokenCount(text) {
        if (!text) return 0;

        text = text.replace(/\s+/g, ' ').trim();
        
        let tokenCount = 0;

        const sentences = text.split(/([.!?]+)/g);
        for (const sentence of sentences) {
            if (!sentence.trim()) continue;
            
            const urlMatches = sentence.match(/https?:\/\/[^\s]+/g) || [];
            for (const url of urlMatches) {
                tokenCount += this._processUrl(url);
                text = text.replace(url, ' ');
            }

            const words = sentence.trim().split(/\s+/);
            
            for (const word of words) {
                const emojiMatches = word.match(/\p{Emoji_Presentation}|\p{Extended_Pictographic}/gu) || [];
                if (emojiMatches.length > 0) {
                    tokenCount += this._processEmojis(emojiMatches);
                    continue;
                }

                if (/[ê°€-í£ã„±-ã…ã…-ã…£]/.test(word)) {
                    tokenCount += this._processKorean(word);
                    continue;
                }

                if (/[a-zA-Z0-9]/.test(word)) {
                    tokenCount += this._processAlphanumeric(word);
                    continue;
                }

                tokenCount += this._processSpecialChars(word);
            }
        }

        return Math.max(1, Math.round(tokenCount * 1.05));
    }

    /**
     * @description ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬ ì´ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.
     * ëŒ€í™” ë‚´ì—­ì´ë‚˜ ì—¬ëŸ¬ ë¬¸ì„œì˜ ì´ í† í° ìˆ˜ë¥¼ í™•ì¸í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
     * 
     * @param {Array<string>} texts - í† í° ìˆ˜ë¥¼ ê³„ì‚°í•  í…ìŠ¤íŠ¸ ë°°ì—´
     *   - ë¹ˆ ë¬¸ìì—´ì´ë‚˜ null ê°’ì´ í¬í•¨ëœ ê²½ìš° 0ìœ¼ë¡œ ì²˜ë¦¬ë¨
     *   - ë°°ì—´ì´ ë¹„ì–´ìˆëŠ” ê²½ìš° 0ì„ ë°˜í™˜
     * 
     * @returns {number} ëª¨ë“  í…ìŠ¤íŠ¸ì˜ ì´ í† í° ìˆ˜ í•©ê³„
     * 
     * @throws {TypeError} textsê°€ ë°°ì—´ì´ ì•„ë‹Œ ê²½ìš°
     * 
     * @see getEstimatedTokenCount - ê°œë³„ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°ì— ì‚¬ìš©ë˜ëŠ” ë©”ì„œë“œ
     * 
     * @example ëŒ€í™” ë‚´ì—­ì˜ ì´ í† í° ìˆ˜ ê³„ì‚°
     * const messages = [
     *   "ì•ˆë…•í•˜ì„¸ìš”!",
     *   "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”.",
     *   "ë„¤, ì‚°ì±…í•˜ê¸° ì¢‹ì€ ë‚ ì”¨ì…ë‹ˆë‹¤. ğŸ˜Š"
     * ];
     * TokenCounter.getTotalEstimatedTokenCount(messages);
     * // ê²°ê³¼: 12
     * 
     * @example ë¹ˆ ê°’ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ ì²˜ë¦¬
     * const texts = ["Hello", "", null, "World"];
     * TokenCounter.getTotalEstimatedTokenCount(texts);
     * // ê²°ê³¼: 2
     * 
     * @note
     * - ê° í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ëŠ” getEstimatedTokenCount ë©”ì„œë“œë¥¼ í†µí•´ ê³„ì‚°ë¨
     * - OpenAI APIì˜ ì‹¤ì œ í† í° ìˆ˜ì™€ ì•½ 3% ë‚´ì™¸ì˜ ì˜¤ì°¨ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ
     */
    static getTotalEstimatedTokenCount(texts) {
        return texts.reduce((sum, text) => 
            sum + this.getEstimatedTokenCount(text), 0);
    }

    /**
     * @description í…ìŠ¤íŠ¸ê°€ ì§€ì •ëœ í† í° ì œí•œì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.
     * í…ìŠ¤íŠ¸ ë¶„í• ì´ë‚˜ ìµœì í™”ê°€ í•„ìš”í•œì§€ íŒë‹¨í•˜ê¸° ìœ„í•œ ì‚¬ì „ ê²€ì‚¬ì— í™œìš©ë©ë‹ˆë‹¤.
     * 
     * @param {string} text - í† í° ìˆ˜ë¥¼ í™•ì¸í•  í…ìŠ¤íŠ¸
     *   - ë¹ˆ ë¬¸ìì—´ì´ë‚˜ nullì˜ ê²½ìš° false ë°˜í™˜
     *   - URL, ì´ëª¨ì§€, í•œê¸€, ì˜ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì ë“± ëª¨ë“  ìœ í˜•ì˜ í…ìŠ¤íŠ¸ ì§€ì›
     * @param {number} limit - í† í° ì œí•œ ìˆ˜
     *   - 1 ì´ìƒì˜ ì •ìˆ˜ì—¬ì•¼ í•¨
     *   - ì‹¤ì œ ì‚¬ìš©ì‹œ ëª¨ë¸ë³„ ì œí•œ(ì˜ˆ: GPT-3.5ëŠ” 4096) ê³ ë ¤ í•„ìš”
     * 
     * @returns {boolean} í† í° ì œí•œ ì´ˆê³¼ ì—¬ë¶€
     *   - true: í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ê°€ ì œí•œì„ ì´ˆê³¼í•¨
     *   - false: í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ê°€ ì œí•œ ì´í•˜ì„
     * 
     * @throws {TypeError} limitê°€ ìˆ«ìê°€ ì•„ë‹ˆê±°ë‚˜ 1 ë¯¸ë§Œì¸ ê²½ìš°
     * 
     * @see getEstimatedTokenCount - í† í° ìˆ˜ ê³„ì‚°ì— ì‚¬ìš©ë˜ëŠ” ë©”ì„œë“œ
     * @see splitByTokenLimit - ì œí•œì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš° í…ìŠ¤íŠ¸ ë¶„í• ì— ì‚¬ìš©
     * @see optimizeToTokenLimit - ì œí•œì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš° í…ìŠ¤íŠ¸ ìµœì í™”ì— ì‚¬ìš©
     * 
     * @example ê¸°ë³¸ ì‚¬ìš©ë²•
     * const text = "ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.";
     * TokenCounter.exceedsTokenLimit(text, 10);
     * // ê²°ê³¼: false (í† í° ìˆ˜ê°€ 10 ì´í•˜ì¸ ê²½ìš°)
     * 
     * @example í…ìŠ¤íŠ¸ ë¶„í•  ì „ ê²€ì‚¬
     * const longText = "ì´ê²ƒì€ ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤...";
     * if (TokenCounter.exceedsTokenLimit(longText, 5)) {
     *   const chunks = TokenCounter.splitByTokenLimit(longText, 5);
     * }
     * 
     * @note
     * - í† í° ìˆ˜ ê³„ì‚°ì€ ì¶”ì •ì¹˜ì´ë©°, ì‹¤ì œ OpenAI API ì‚¬ìš©ì‹œì˜ í† í° ìˆ˜ì™€ ì•½ 3% ë‚´ì™¸ì˜ ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
     * - ì¤‘ìš”í•œ ì²˜ë¦¬ì˜ ê²½ìš° ì—¬ìœ ìˆëŠ” ì œí•œê°’ ì„¤ì •ì„ ê¶Œì¥í•©ë‹ˆë‹¤
     */
    static exceedsTokenLimit(text, limit) {
        return this.getEstimatedTokenCount(text) > limit;
    }

    /**
     * @description ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ í† í° ì œí•œì— ë§ì¶° ì—¬ëŸ¬ ê°œì˜ ì‘ì€ í…ìŠ¤íŠ¸ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
     * ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ ìµœëŒ€í•œ ë³´ì¡´í•˜ë©´ì„œ ê° ë¶€ë¶„ì´ ì§€ì •ëœ í† í° ìˆ˜ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ë¶„í• í•©ë‹ˆë‹¤.
     * 
     * @param {string} text - ë¶„í• í•˜ê³ ì í•˜ëŠ” ì›ë³¸ í…ìŠ¤íŠ¸
     *   - ë¹ˆ ë¬¸ìì—´ì´ë‚˜ nullì´ ì…ë ¥ë˜ë©´ ë¹ˆ ë°°ì—´ ë°˜í™˜
     *   - ë¬¸ì¥ êµ¬ë¶„ì(., !, ?)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• ë¨
     * @param {number} maxTokens - ê° ë¶„í• ëœ í…ìŠ¤íŠ¸ì˜ ìµœëŒ€ í† í° ìˆ˜
     *   - 1 ì´ìƒì˜ ì •ìˆ˜ì—¬ì•¼ í•¨
     *   - ë„ˆë¬´ ì‘ì€ ê°’ì„ ì§€ì •í•˜ë©´ ë¬¸ì¥ì´ ì˜ë„ì¹˜ ì•Šê²Œ ë¶„í• ë  ìˆ˜ ìˆìŒ
     * 
     * @returns {Array<string>} ë¶„í• ëœ í…ìŠ¤íŠ¸ ë°°ì—´
     *   - ê° ìš”ì†ŒëŠ” maxTokens ì´í•˜ì˜ í† í°ì„ ê°€ì§
     *   - ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ìˆœì„œê°€ ë³´ì¡´ë¨
     * 
     * @throws {TypeError} maxTokensê°€ ìˆ«ìê°€ ì•„ë‹ˆê±°ë‚˜ 1 ë¯¸ë§Œì¸ ê²½ìš°
     * 
     * @see getEstimatedTokenCount - ê° ë¶„í• ëœ í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°ì— ì‚¬ìš©
     * @see optimizeToTokenLimit - í…ìŠ¤íŠ¸ë¥¼ íŠ¹ì • í† í° ìˆ˜ì— ìµœì í™”í•  ë•Œ í•¨ê»˜ ì‚¬ìš©ë¨
     * 
     * @example ê¸´ í…ìŠ¤íŠ¸ ë¶„í• 
     * const text = "ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”! AIëŠ” ì •ë§ í¥ë¯¸ë¡­ìŠµë‹ˆë‹¤.";
     * TokenCounter.splitByTokenLimit(text, 5);
     * // ê²°ê³¼: ["ì•ˆë…•í•˜ì„¸ìš”.", "ì˜¤ëŠ˜ì€ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”!", "AIëŠ” ì •ë§ í¥ë¯¸ë¡­ìŠµë‹ˆë‹¤."]
     * 
     * @example ìµœì í™”ì™€ í•¨ê»˜ ì‚¬ìš©
     * const longText = "ì´ê²ƒì€ ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤...";
     * const chunks = TokenCounter.splitByTokenLimit(longText, 10);
     * const optimizedChunks = chunks.map(chunk => 
     *   TokenCounter.optimizeToTokenLimit(chunk, 8)
     * );
     * 
     * @note
     * - ë¬¸ì¥ ì¤‘ê°„ì—ì„œ ë¶„í• ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë¬¸ì¥ êµ¬ë¶„ì(., !, ?)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
     * - ë‹¨ì¼ ë¬¸ì¥ì´ maxTokensë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°ì—ë„ í•´ë‹¹ ë¬¸ì¥ì€ ë¶„í• ë˜ì§€ ì•ŠìŒ
     * - ë°˜í™˜ëœ ê° í…ìŠ¤íŠ¸ëŠ” ì•ë’¤ ê³µë°±ì´ ì œê±°ëœ ìƒíƒœ
     */
    static splitByTokenLimit(text, maxTokens) {
        const chunks = [];
        const sentences = text.split(/([.!?]+)/g);
        let currentChunk = '';
        
        for (const sentence of sentences) {
            const tempChunk = currentChunk + sentence;
            if (this.getEstimatedTokenCount(tempChunk) <= maxTokens) {
                currentChunk = tempChunk;
            } else {
                if (currentChunk) chunks.push(currentChunk.trim());
                currentChunk = sentence;
            }
        }
        
        if (currentChunk) chunks.push(currentChunk.trim());
        return chunks;
    }

    /**
     * @description ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ í† í° ì œí•œì— ë§ì¶”ì–´ ìµœì í™”í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.
     * URL ì¶•ì•½, ê³µë°± ì œê±°, í…ìŠ¤íŠ¸ ì˜ë¼ë‚´ê¸° ë“±ì˜ ë°©ë²•ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì••ì¶•í•©ë‹ˆë‹¤.
     * 
     * @param {string} text - ìµœì í™”í•  í…ìŠ¤íŠ¸
     *   - URL, ì¼ë°˜ í…ìŠ¤íŠ¸ ë“± ëª¨ë“  í˜•ì‹ì˜ í…ìŠ¤íŠ¸ ì§€ì›
     *   - ë¹ˆ ë¬¸ìì—´ì´ë‚˜ nullì˜ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
     * @param {number} targetTokens - ëª©í‘œ í† í° ìˆ˜
     *   - 1 ì´ìƒì˜ ì •ìˆ˜ì—¬ì•¼ í•¨
     *   - í…ìŠ¤íŠ¸ê°€ ì´ í† í° ìˆ˜ë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ìµœì í™”ë¨
     * 
     * @returns {string} ìµœì í™”ëœ í…ìŠ¤íŠ¸
     *   - ëª©í‘œ í† í° ìˆ˜ ì´í•˜ë¡œ ìµœì í™”ëœ í…ìŠ¤íŠ¸
     *   - í…ìŠ¤íŠ¸ê°€ ì˜ë¦° ê²½ìš° ëì— '...' ì¶”ê°€
     * 
     * @throws {TypeError} targetTokensê°€ ìˆ«ìê°€ ì•„ë‹ˆê±°ë‚˜ 1 ë¯¸ë§Œì¸ ê²½ìš°
     * 
     * @see getEstimatedTokenCount - í† í° ìˆ˜ ê³„ì‚°ì— ì‚¬ìš©
     * @see splitByTokenLimit - í…ìŠ¤íŠ¸ ë¶„í• ì´ í•„ìš”í•œ ê²½ìš° í•¨ê»˜ ì‚¬ìš©
     * 
     * @example ê¸°ë³¸ í…ìŠ¤íŠ¸ ìµœì í™”
     * const text = "ì´ê²ƒì€ ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤...";
     * TokenCounter.optimizeToTokenLimit(text, 5);
     * // ê²°ê³¼: "ì´ê²ƒì€ ë§¤ìš°..."
     * 
     * @example URLì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ ìµœì í™”
     * const text = "ìì„¸í•œ ë‚´ìš©ì€ https://example.com/very/long/path/to/document ì°¸ê³ ";
     * TokenCounter.optimizeToTokenLimit(text, 8);
     * // ê²°ê³¼: "ìì„¸í•œ ë‚´ìš©ì€ https://example.com/... ì°¸ê³ "
     * 
     * @note
     * - URLì€ 30ì ì´ìƒì¸ ê²½ìš° ë„ë©”ì¸ë§Œ ë‚¨ê¸°ê³  ì¶•ì•½ë¨
     * - ì—°ì†ëœ ê³µë°±ì€ ë‹¨ì¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜ë¨
     * - ìµœì í™” í›„ì—ë„ í† í° ì œí•œì„ ì´ˆê³¼í•˜ëŠ” ê²½ìš° í…ìŠ¤íŠ¸ê°€ ì˜ë¦¼
     */
    static optimizeToTokenLimit(text, targetTokens) {
        if (this.getEstimatedTokenCount(text) <= targetTokens) {
            return text;
        }

        text = text.replace(/(https?:\/\/[^\s]{30,})/g, (url) => {
            try {
                const urlObj = new URL(url);
                return `${urlObj.protocol}//${urlObj.hostname}/...`;
            } catch {
                return url;
            }
        });

        text = text.replace(/\s+/g, ' ');

        if (this.getEstimatedTokenCount(text) > targetTokens) {
            const words = text.split(/\s+/);
            let result = '';
            for (const word of words) {
                const temp = result + ' ' + word;
                if (this.getEstimatedTokenCount(temp) <= targetTokens) {
                    result = temp;
                } else {
                    break;
                }
            }
            return result.trim() + '...';
        }

        return text;
    }


    static _processUrl(url) {
        let count = 0;

        for (const [pattern, tokenValue] of Object.entries(this.urlPatterns)) {
            if (url.includes(pattern)) {
                count += tokenValue;
            }
        }

        const remainingParts = url
            .replace(/https?:\/\//g, '')
            .replace(/\.(com|net|org|io|ai|gov|edu)/g, '')
            .split(/[/\-_]/);
        
        for (const part of remainingParts) {
            if (part.length > 0) {
                count += Math.ceil(part.length / 4);
            }
        }

        return Math.max(1, count);
    }

    static _processEmojis(emojis) {
        let count = 0;

        let prevEmoji = '';
        for (const emoji of emojis) {
            count += 1;

            if (prevEmoji) count += 0.5;
            prevEmoji = emoji;
        }

        return Math.ceil(count);
    }

    static _processKorean(word) {        
        let count = 0;

        const completeChars = word.match(/[ê°€-í£]+/g) || [];
        for (const chars of completeChars) {
            count += Math.ceil(chars.length * 0.6);

            if (chars.length > 4) {
                count += Math.floor(chars.length / 4);
            }
        }

        const jamoChars = word.match(/[ã„±-ã…ã…-ã…£]+/g) || [];
        for (const jamo of jamoChars) {
            count += Math.ceil(jamo.length * 0.3);
        }

        return Math.max(1, count);
    }

    static _processAlphanumeric(word) {
        let count = 0;

        const numbers = word.match(/\d+/g) || [];
        for (const num of numbers) {
            count += Math.max(1, Math.ceil(num.length / 6));
        }

        const englishParts = word.match(/[a-zA-Z]+/g) || [];
        for (const part of englishParts) {
            const hasUpperCase = /[A-Z]/.test(part);
            if (part.length <= 4) {
                count += 1;
            } else {
                count += Math.ceil(part.length / (hasUpperCase ? 3 : 4));
            }
        }

        return count;
    }

    static _processSpecialChars(word) {
        const specialChars = word.match(/[^\w\sê°€-í£ã„±-ã…ã…-ã…£\p{Emoji_Presentation}\p{Extended_Pictographic}]/gu) || [];
        let count = 0;
        let consecutive = 0;

        for (let i = 0; i < specialChars.length; i++) {
            if (i > 0 && specialChars[i] === specialChars[i-1]) {
                consecutive++;
            } else {
                consecutive = 0;
            }
            count += 1 / (consecutive + 1);
        }

        return Math.ceil(count);
    }
}

TokenCounter.urlPatterns = {
    'http://': 1,
    'https://': 1,
    'www.': 1,
    '.com': 1,
    '.net': 1,
    '.org': 1,
    '.io': 1,
    '.ai': 1,
    '.gov': 1,
    '.edu': 1
};

module.exports = TokenCounter;